/* ParseVCF.js
 * In order to complete the file upload into the database, the output from 
 * Annovar must be parsed. To ensure the fastest possible parsing and more
 * Memory efficient method, this module was written.
 *
 * THe parseVCF object taktes two parameters when initiated: file and patients.
 * with severa; optional parameteres following. Some vcf files can be extemeely
 * larege, therefore to facilitate file reading in an async manner, the parser
 * splits the file into chunks inserts them into a buffer, and then maps the 
 * parsed output to the header fields. Additionally, If multiple patients are 
 * provided, the parser automatically bulds the documents for each patient.
 * When the number of documents for each patient reaches just under 1000 (The current max)
 * of insertMany, the parser will attempt to insert the documents into the database.
 * It then clears the current document string to save memory
 * 
 * when the file reading is completed, all remaining documents are inserted and the file is
 * closed.
 * 
 *
 * written by Patrick Magee
*/
var Promise = require('bluebird');
var fs = Promise.promisifyAll(require('fs'));
var dbFunctions = require('./mongodb_functions');

/* Empty constructor for the parseVCF object */
var parseVCF = function(file,patients,bufferSize){
	this.file = file;
	this.patients = patients;
	this.dbFunctions = dbFunctions;
	this.bufferSize = (bufferSize || 10000000);
	this.bufferArray = []
	//this.fd;
	this.oldString = "";
	this.mapper = {};
	this.first = true;
	this.firstEntry = true;
	this.docMax = 5000;
	this.patientObj = {};
	//this.chunkArray = [];
	this.stream;
	this.reading = false;
};
//==============================================================================================================
/* Create a read stream to read data from, and add event handlers to 
 * specific events. The read stream will pass chunks of a specific size
 * to the data field and drain the buffer. When incoming data is receieved
 * the stream is paused, and the chunk is added to an array. This is done 
 * because despite the stream being paused data could potentially be in the
 * process of being added. The bufferArray is then parsed and emptied using
 * the readAndParse function.
 * finally when the end of the file has been reached the remaining files are
 * inserted into the document
 */
parseVCF.prototype.read = function(){
	var self = this;
	var promise = new Promise(function(resolve,reject){
		self.stream = fs.createReadStream(self.file,{'bufferSize':this.bufferSize});
		self.stream.on('data',function(data){
			self.stream.pause();
			var promise = new Promise(function(resolve,reject){
				self.bufferArray.push(data);
				if (!self.reading){
					self.reading = true;
					self.readAndParse()
					.then(function(){
						self.stream.resume();
						self.reading = false;
						resolve(self.stream)
					});
				} else {
					resolve(self.stream);
				}
			});
			return promise;

		});

		self.stream.on('end',function(){
			Promise.each(Object.keys(self.patientObj),function(patient){
				return self.checkAndInsertDoc(patient);
			}).then(function(){
				var igArray = []
				for (patient in self.patientObj){
					if (self.patientObj.hasOwnProperty(patient)){
						igArray.push(self.patientObj[patient]['ignored']);
					};
				}
				resolve(igArray)
			});
		});

		self.stream.on('error',function(err){
			//self.stream.destroy();
			reject(err)
		});
	});
	return promise
};
//==============================================================================================================
/* ONce the file has been open for reading, a new buffer is initialized and 
 * passed to the fs.readAsync function. It then converts the string back to utf-8 
 * encoding, checks to see if the string is a continuation of the old string, and 
 * then splits it into an array based on the new line character.
 * it then makes a call to the main parser function and upon reaching a specified number
 * of documents for each patients, inserts them into the connected database
 * 
 * returns a promise
 */
parseVCF.prototype.readAndParse = function(chunk){
	var self = this;	
	var promise = new Promise(function(resolve,reject){
		var chunk = self.bufferArray[0];
		self.bufferArray.shift();
		Promise.resolve().then(function(){
			return chunk.toString('utf-8');
		}).then(function(string){
			if (self.oldString != ""){
				string = self.oldString + string;
			}
			splitString = string.split('\n');
			if (string.substr(string.length - 1) == "\n"){
				self.oldString = "";
			}  else {
				self.oldString = splitString.pop();
			}
			return splitString;
		}).then(function(stringArray){
			if (stringArray.length > 0 )
				return self.parseChunk(stringArray).map(function(patient){
					if (self.patientObj[patient]['documents'].length >= self.docMax)
						return self.checkAndInsertDoc(patient);
				});
		}).then(function(){
			if (self.bufferArray.length > 0){
				return self.readAndParse();
			}
		}).then(function(){
			resolve('read chunk');
		}).catch(function(err){
			reject(err);
		});
	});
	return promise;
};



//==============================================================================================================
/* Annovar multi-anno parser. Takes an array as input that contains lines separated by the
 * new line character. It then breaks the line into its an array by splitting on tabbed or
 * white space. It uses a mapper to link the headers with specific rows, as well as the mapped
 * patients indexes. It then outputs an object that it pushes to each patient in the 
 * this.patientObj.documents array.
 *
 * return a promise, and an array of patient keys
 */
parseVCF.prototype.parseChunk = function(stringArray){
	var self = this;
	var promise = new Promise(function(resolve,reject){
		//iterate over all strings that are included
		for (var i=0; i < stringArray.length ; i++ ){
			//If the file is malformed, it may have ##, ignore these
			if (stringArray[i] != "" && stringArray[i].slice(0,2) != "##"){
				line = stringArray[i].split('\t'); //split string
				//If this is the first line that has been read, extract the file headers
				if (self.first){
					line = line.filter(function(ele){
						if (ele != "Otherinfo")
							return ele; //return all but the Otherinfo field. This is not informative
					});
					for (var j=0; j<line.length; j++)
					{
						//add each field to the mapper, replacing periods or hashtags. the value is the corresponding columns
						self.mapper[line[j].replace(/\./gi,'_').replace(/#/i,"").toLowerCase()] = j;
					}
					self.first = false;
				} else {
					if (self.firstEntry){ //first record
						var formatReached = false
						var formatNum;
						//iterate over the broken up line to find the rs number and the FORMATFIELD
						for (var j=0; j<line.length;j++){
							if (line[j].match(/^rs[0-9]+$/)){
								self.mapper['identifier'] = j;
							} else if (line[j].match(/^GT+/)) {
								self.mapper['FORMATFIELD'] = j;
								formatReached = true;
								formatNum = j + 1;
							} else if (formatReached) {
								//once formatField is reached, add the indexes of the patients to the patientObj
								self.patientObj[self.patients[j - formatNum]['patient_id']] = {'id':j,
													'collection':self.patients[j - formatNum]['collection_id'],
													'documents':[],
													'ignored':0};
								
							}	
						}
						self.firstEntry = false;
					}
					
					//using the mapper. add each field as an entry for each patient in the patientObj
					var formatMapper = [];
					var formatField = line[self.mapper['FORMATFIELD']].split(':');
					for (patient in self.patientObj){
						if (self.patientObj.hasOwnProperty(patient)){
							var currDoc = {};
							var formatLine = line[self.patientObj[patient]['id']].split(':');
							var cont = true;
							//extract information about the format of the current entry (can occasionally change so this)
							//Must be done each time.
							for (var j = 0; j < formatField.length; j++){
								var info = formatLine[j].split(/[\/|,]/);
								info = info.map(function(item){
									if (isNaN(item))
										return item;
									else
										return Math.round(item*1000)/1000;
								});
								if (formatField[j] == 'GT'){
									if (info.indexOf('.') != -1){
										self.patientObj[patient]['ignored'] += 1
										cont = false;
									} else {
										currDoc['gt-raw'] = formatLine[j];
										currDoc['gt'] = info;
										currDoc['phased_status'] = (formatLine[j].indexOf('|') != -1 || false);
									}
								} else if (cont){
									currDoc[formatField[j].toLowerCase()] = (info.length == 1 ? info[0]:info);
								}
							} // if the genotype given is ./. ie, there is nothing found for the patient, don't insert it
							  // Posibbly change to 0/0 ref allele later, and imply that any fields not included are ref/ref
							  // however that may be a bad idea
							if (cont) { 
								for (field in self.mapper){
									if (self.mapper.hasOwnProperty(field)){
										var thisLine = line[self.mapper[field]].split(/,/);
										if (field !== 'FORMATFIELD' && thisLine != '.'){
											//insert the other fields into the document
											if (field !== 'chr'){
												var finalLine = thisLine.map(function(item){
													if (isNaN(item)){
														return item;
													} else { 
														return Math.round(item*1000)/1000; //convert to 3 decimal places
																					  //Add function later to change this
													}
												});
											} else {
												var finalLine = thisLine;
											}
											currDoc[field] = (finalLine.length == 1 ? finalLine[0]:finalLine);
										}
									}
								}
								//when completed push this to the docuements array for each patient.
								self.patientObj[patient]['documents'].push(currDoc);
							}
						}
					}
				}
			}
		}
		resolve(Object.keys(self.patientObj));
	});
	return promise;
};

//==============================================================================================================
/* Upon completetion of the parsing, add any remaining entries into
 * the database. recursively calling itself until all entries are in
 */
parseVCF.prototype.cleanup = function(patient){
	var self = this;
	return this.checkAndInsertDoc(patient)
	.then(function(){
		if (self.patientObj[patient]['documents'].length > 0){
			self.cleanup(patient);
		}
	})
};

//==============================================================================================================
/* Insert up to this.docMax entries (default 999) into the connected database
 * for the specified patient. Additionally, remove from memory the documents 
 * that were inserted
 * returns a promise

 */
parseVCF.prototype.checkAndInsertDoc = function(patient){
	var self = this;
	var promise = new Promise(function(resolve,reject){
		var docsToInsert = self.patientObj[patient]['documents'];
		var options = {documents: docsToInsert,
				   collectionName:self.patientObj[patient]['collection']};
		
		dbFunctions.insertMany(options)
		.then(function(){
			self.patientObj[patient]['documents'] = []//self.patientObj[patient]['documents'].slice(ind);
			resolve(self.patientObj[patient]['documents'].length)
		}).catch(function(err){
			console.log(err.stack);
		});
	});
	return promise;
};


//==============================================================================================================
//==============================================================================================================
// Run with command line arguments
//==============================================================================================================
//==============================================================================================================
var output;
var fileName = process.argv[2];
var patients = JSON.parse(process.argv[3]); 
return dbFunctions.connectAndInitializeDB(true)
	.then(function(){
		var parser = new parseVCF(fileName,patients);
		return parser.read().then(function(result){
			console.log(JSON.stringify(result)); // print to console the ignoreArray, for capture by the parent process
			output = result;
		});
	}).catch(function(err){
		console.log(err.toString());
	}).done(function(){
		return dbFunctions.closeConnection();
	});

//module.exports = parseVCF;


