/* ParseVCF.js
 * In order to complete the file upload into the database, the output from 
 * Annovar must be parsed. To ensure the fastest possible parsing and more
 * Memory efficient method, this module was written.
 *
 * THe parseVCF object taktes two parameters when initiated: file and patients.
 * with severa; optional parameteres following. Some vcf files can be extemeely
 * larege, therefore to facilitate file reading in an async manner, the parser
 * splits the file into chunks inserts them into a buffer, and then maps the 
 * parsed output to the header fields. Additionally, If multiple patients are 
 * provided, the parser automatically bulds the documents for each patient.
 * When the number of documents for each patient reaches just under 1000 (The current max)
 * of insertMany, the parser will attempt to insert the documents into the database.
 * It then clears the current document string to save memory
 * 
 * when the file reading is completed, all remaining documents are inserted and the file is
 * closed.
 * 
 *
 * written by Patrick Magee
*/
var Promise = require('bluebird');
var fs = Promise.promisifyAll(require('fs'));
var dbFunctions = require('./mongodb_functions');

/* Empty constructor for the parseVCF object */
var parseVCF = function(file,patients,bufferSize,mask){
	this.file = file;
	this.patients = patients;
	this.dbFunctions = dbFunctions;
	this.bufferSize = (bufferSize || 10000000);
	this.bufferArray = []
	//this.fd;
	this.oldString = "";
	this.mapper = {'static':{},'anno':[]};
	this.docMax = 5000;
	this.patientObj = {};
	//this.chunkArray = [];
	this.stream;
	this.reading = false;
	this.mask = (mask || ['qual','filter']);
	this.numHeader = 0
};
//==============================================================================================================
/* Create a read stream to read data from, and add event handlers to 
 * specific events. The read stream will pass chunks of a specific size
 * to the data field and drain the buffer. When incoming data is receieved
 * the stream is paused, and the chunk is added to an array. This is done 
 * because despite the stream being paused data could potentially be in the
 * process of being added. The bufferArray is then parsed and emptied using
 * the readAndParse function.
 * finally when the end of the file has been reached the remaining files are
 * inserted into the document
 */
parseVCF.prototype.read = function(){
	var self = this;
	var promise = new Promise(function(resolve,reject){
		self.stream = fs.createReadStream(self.file,{'bufferSize':this.bufferSize});
		self.stream.on('data',function(data){
			self.stream.pause();
			var promise = new Promise(function(resolve,reject){
				self.bufferArray.push(data);
				if (!self.reading){
					self.reading = true;
					self.readAndParse()
					.then(function(){
						self.stream.resume();
						self.reading = false;
						resolve(self.stream)
					});
				} else {
					resolve(self.stream);
				}
			});
			return promise;

		});

		self.stream.on('end',function(){
			Promise.each(Object.keys(self.patientObj),function(patient){
				return self.checkAndInsertDoc(patient);
			}).then(function(){
				var igArray = []
				for (patient in self.patientObj){
					if (self.patientObj.hasOwnProperty(patient)){
						igArray.push(self.patientObj[patient]['ignored'] + self.numHeader);
					};
				}
				resolve(igArray)
			});
		});

		self.stream.on('error',function(err){
			//self.stream.destroy();
			reject(err)
		});
	});
	return promise
};
//==============================================================================================================
/* ONce the file has been open for reading, a new buffer is initialized and 
 * passed to the fs.readAsync function. It then converts the string back to utf-8 
 * encoding, checks to see if the string is a continuation of the old string, and 
 * then splits it into an array based on the new line character.
 * it then makes a call to the main parser function and upon reaching a specified number
 * of documents for each patients, inserts them into the connected database
 * 
 * returns a promise
 */
parseVCF.prototype.readAndParse = function(chunk){
	var self = this;	
	var promise = new Promise(function(resolve,reject){
		var chunk = self.bufferArray[0];
		self.bufferArray.shift();
		Promise.resolve().then(function(){
			return chunk.toString('utf-8');
		}).then(function(string){
			if (self.oldString != ""){
				string = self.oldString + string;
			}
			splitString = string.split('\n');
			if (string.substr(string.length - 1) == "\n"){
				self.oldString = "";
			}  else {
				self.oldString = splitString.pop();
			}
			return splitString;
		}).then(function(stringArray){
			if (stringArray.length > 0 )
				return self.parseChunk(stringArray).map(function(patient){
					if (self.patientObj[patient]['documents'].length >= self.docMax)
						return self.checkAndInsertDoc(patient);
			});
		}).then(function(){
			if (self.bufferArray.length > 0){
				return self.readAndParse();
			}
		}).then(function(){
			resolve('read chunk');
		}).catch(function(err){
			reject(err);
		});
	});
	return promise;
};



//==============================================================================================================
/* Annovar multi-anno parser. Takes an array as input that contains lines separated by the
 * new line character. It then breaks the line into its an array by splitting on tabbed or
 * white space. It uses a mapper to link the headers with specific rows, as well as the mapped
 * patients indexes. It then outputs an object that it pushes to each patient in the 
 * this.patientObj.documents array.
 *
 * return a promise, and an array of patient keys
 */
parseVCF.prototype.parseChunk = function(stringArray){
	var self = this;
	var promise = new Promise(function(resolve,reject){
		//iterate over all strings that are include
		for (var i=0; i < stringArray.length ; i++ ){
			//If the file is malformed, it may have ##, ignore these
			if (stringArray[i] != "" ){
				if(stringArray[i].startsWith('##INFO') && stringArray[i].match(/annovar/i)!=null){
					var line = stringArray[i].toLowerCase().match(/id=[a-z0-9\.-_]+/i)[0].replace('id=','').replace('.','_');
					self.mapper['anno'].push(line);
					self.numHeader++
				} else if (stringArray[i].startsWith("#CHR")) {
					self.numHeader++
					var formatReached = false;

					var staticLine = stringArray[i].toLowerCase().split('\t')
					for (var j = 0; j < staticLine.length; j++ ){
						if(self.mask.indexOf(staticLine[j]) == -1){
							if (staticLine[j] == 'format'){
								self.mapper['format'] = j;
								formatReached = true;

							} else if (formatReached) {
								self.patientObj[self.patients[j - self.mapper.format - 1]['patient_id']] = {'id':j,
											'collection':self.patients[j - self.mapper.format - 1]['collection_id'],
											'documents':[],
											'ignored':0,
											'insertCache':[]};

							} else if (staticLine[j] == 'info'){
								self.mapper['annofield'] = j
							} else {
								self.mapper.static[staticLine[j].replace('#','')] = j;
 							}
 						}
					}
				} else if (!stringArray[i].startsWith('#')) {
					var line = stringArray[i].toLowerCase().split('\t');
					var annoObj = self.convertAnnoString(line[self.mapper.annofield]);
					var annoList = self.mapper.anno;
					//loop over all of the patients included
					for (patient in self.patientObj){
						var cont = true; 
						var currDoc = {};
						//check to make sure that patient exists
						if (self.patientObj.hasOwnProperty(patient)){
							//loop over all of the static fields ie. chr pos etc
							for (field in self.mapper.static){
								if (self.mapper.static.hasOwnProperty(field)){
									var itemToInsert = line[self.mapper.static[field]].split(',');
									if (field.match('chr')==null){ // we want to keep chr as a string so dont convert it
										itemToInsert = itemToInsert.map(convertNum);
									}
									//if the lenght of the final array is > 1 then insert the array otherwise insert the item
									currDoc[field] = (itemToInsert.length > 1 ? itemToInsert:itemToInsert[0]);
								}
							}
							//Loop over all the items in the annovar annotation list
							for (var j=0; j < annoList.length; j++){
								//if the annotation is the annovar_date or annovar_end then dont include it
								if (annoList[j].search(/annovar(\.|_)date/i) == -1 && 
										annoList[j].search(/allele(\.|_)end/i) == -1){
									//currDoc[annoList[j]] = annoObj
									var itemToInsert = annoObj[annoList[j]];
									var itemToInsert = itemToInsert.map(function(item){
										if (item !== "."){
											item = item.split(',');
											return (item.length > 1 ? item:item[0]);
										}
									});	
									if (countUndefined(itemToInsert)){
										currDoc[annoList[j]] = (itemToInsert.length > 1 ? itemToInsert:itemToInsert[0]);
									}
								}
							}
						}

						//Add the format fields now, these are additional information including the genotype
						var formatMapper = [];
						var formatField = line[self.mapper.format].split(':');
						var formatLine = line[self.patientObj[patient]['id']].split(':');
						for (var j = 0; j < formatField.length; j++ ){
							var info = formatLine[j].split(/[\/|,]/);
							info = info.map(convertNum)

							if (formatField[j] == 'gt'){
								if (info.indexOf('.') != -1){
									self.patientObj[patient]['ignored']++;
									cont = false;
								} else {
									currDoc['zygosity'] = zygosity(info);
									currDoc['gt-raw'] = formatLine[j];
									currDoc[formatField[j]] = info;
									currDoc['phased_status'] = (formatLine[j].indexOf('|') != -1 || false);
								}
							} else if (cont){
								currDoc[formatField[j]] = (info.length == 1 ? info[0]:info);
							}
						}
						if (cont){
							self.patientObj[patient]['documents'].push(currDoc);
						}
					}
				} else {
					self.numHeader++
				}
			}
		}
		resolve(Object.keys(self.patientObj));
	});
	return promise;
};

//==============================================================================================================
/* Upon completetion of the parsing, add any remaining entries into
 * the database. recursively calling itself until all entries are in
 */
parseVCF.prototype.cleanup = function(patient){
	var self = this;
	return this.checkAndInsertDoc(patient)
	.then(function(){
		if (self.patientObj[patient]['documents'].length > 0){
			self.cleanup(patient);
		}
	})
};

//==============================================================================================================
/* Insert up to this.docMax entries (default 999) into the connected database
 * for the specified patient. Additionally, remove from memory the documents 
 * that were inserted
 * returns a promise

 */
parseVCF.prototype.checkAndInsertDoc = function(patient){
	var self = this;
	var promise = new Promise(function(resolve,reject){
		var docsToInsert = self.patientObj[patient]['documents'];
		var options = {documents: docsToInsert,
				   collectionName:self.patientObj[patient]['collection']};
		
		dbFunctions.insertMany(options)
		.then(function(){
			self.patientObj[patient]['documents'] = []//self.patientObj[patient]['documents'].slice(ind);
			self.patientObj[patient]['insertCache'] = [];
			resolve(self.patientObj[patient]['documents'].length)
		}).catch(function(err){
			console.log(err.stack);
		});
	});
	return promise;
};



parseVCF.prototype.convertAnnoString = function(string){
	var out = {};
	var line = string.split(';');
	for (var i = 0; i < line.length; i++ ){
		if ( line[i].match(/^\.$/) == null ){
			var inputLine = line[i].split('=')
			inputLine[0] = inputLine[0].replace('.','_');
			if (!out.hasOwnProperty(inputLine[0])){
				out[inputLine[0]] = [convertNum(inputLine[1])];
			} else {
				out[inputLine[0]].push(convertNum(inputLine[1]));
			}
		}
	}
	return out;
}



//==============================================================================================================
/* helper function for determinging if a funcion is an array. Taken from user Raynos from stack overflow
*/

var isArray = function(obj){
	return Object.prototype.toString.call(obj) === '[object Array]';
}


if (typeof String.prototype.startsWith != 'function') {
   String.prototype.startsWith = function (str){
	   return this.slice(0, str.length) == str;
    };
}

if (typeof String.prototype.endsWith != 'function') {
    String.prototype.endsWith = function (str){
      return this.slice(-str.length) == str;
    };
}


var convertNum = function(str){
	if (isNaN(str)){
		return str;
	} else { 
		return Math.round(str*1000)/1000;
	} 
};

var zygosity = function(arr){
	all1 = arr[0];
	all2 = arr[1];
	if (all1 != all2){
		return 'hetero'
	} else if (all1 == 0){
		return 'homo_ref'
	} else {
		return 'homo_alt'
	}
};

var countUndefined = function(arr){
	var count = 0;
	for (var i=0; i < arr.length; i++ ){
		if (arr[i] == undefined){
			count++
		}
	}
	if (count == arr.length)
		return false
	else
		return true
}


//==============================================================================================================
//==============================================================================================================
// Run with command line arguments
//==============================================================================================================
//==============================================================================================================
var output;
var fileName = process.argv[2];
var patients = JSON.parse(process.argv[3]); 
return dbFunctions.connectAndInitializeDB(true)
	.then(function(){
		var parser = new parseVCF(fileName,patients);
		return parser.read().then(function(result){
			console.log(JSON.stringify(result)); // print to console the ignoreArray, for capture by the parent process
			output = result;
		});
	}).catch(function(err){
		console.log(err.toString());
	}).done(function(){
		return dbFunctions.closeConnection();
	});

//module.exports = parseVCF;
