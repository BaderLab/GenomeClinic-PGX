/* ParseVCF.js
 * In order to complete the file upload into the database, the output from 
 * Annovar must be parsed. To ensure the fastest possible parsing and more
 * Memory efficient method, this module was written.
 *
 * THe parseVCF object taktes two parameters when initiated: file and patients.
 * with severa; optional parameteres following. Some vcf files can be extemeely
 * larege, therefore to facilitate file reading in an async manner, the parser
 * splits the file into chunks inserts them into a buffer, and then maps the 
 * parsed output to the header fields. Additionally, If multiple patients are 
 * provided, the parser automatically bulds the documents for each patient.
 * When the number of documents for each patient reaches just under 1000 (The current max)
 * of insertMany, the parser will attempt to insert the documents into the database.
 * It then clears the current document string to save memory
 * 
 * when the file reading is completed, all remaining documents are inserted and the file is
 * closed.
 * 
 *
 * written by Patrick Magee
*/
var Promise = require('bluebird');
var fs = Promise.promisifyAll(require('fs'));


/* Empty constructor for the parseVCF object */
var parseVCF = function(file,patients,dbFunctions,bufferSize){
	this.file = file;
	this.patients = patients;
	this.dbFunctions = dbFunctions;
	this.bufferSize = (bufferSize || 10000000);
	this.size;
	this.fd;
	this.oldString = "";
	this.mapper = {};
	this.first = true;
	this.firstEntry = true;
	this.docMax = 999;
	this.patientObj = {};
	this.chunkArray = [];
};


/* Initialize the variables and dynamically determine the number of
 * chunks to split the file into, pushing them into an array
 * the chunkArray consists of objects with two properties:
 *
 * chunkLength: the maximum length of the chunk
 * chunkPosition: the position in the file where this chunk starts
 *
 * returns a promise
*/
parseVCF.prototype.init = function(){
	var self = this;
	return fs.statAsync(this.file)
	.then(function(result){
		self.size = result.size;
		for (var i = 0; i < Math.ceil(self.size/self.bufferSize); i++){
			var chunk = {};
			chunk['chunkLength'] = self.bufferSize;
			chunk['chunkPosition'] = i * self.bufferSize;
			self.chunkArray.push(chunk);
		}
	});
};

/* Promisified Open function. This function takes no a arguments,
 * Instead it uses the objects. this.file property, opens the file
 * For reading and returns a promise  after setting the fd property
 * to equal the passed file descriptor
 */
parseVCF.prototype.open = function(){
	var self = this;
	return fs.openAsync(this.file,'r')
	.then(function(fd){
		self.fd = fd;
	});
};


/* Wrapper around the fs.close method 
 * returns a promise */
parseVCF.prototype.close = function(){
	return fs.closeAsync(this.fd);
};

/* Initialize, read, parse and insert the contents of a file into
 * the connected database. This takes the chunksArray made in 
 * the init() function and feeds it to the readAndParse.
 * Each chunk is then read serially into a buffer and then parsed and
 * inserted into the database
 * returns a promise.
 *
 *
 */
parseVCF.prototype.read = function(){
	var self = this;
	return this.init()
	.then(function(){
		return self.open()
	}).then(function(){
		return self.chunkArray;
	}).each(function(chunk){
		return self.readAndParse(chunk);
	}).then(function(){
		return Object.keys(self.patientObj);
	}).each(function(patient){
		return self.cleanup(patient);
	}).then(function(){
		return self.close();
	}).catch(function(err){
		console.log(err.stack)
		self.close();
	});
};


/* ONce the file has been open for reading, a new buffer is initialized and 
 * passed to the fs.readAsync function. It then converts the string back to utf-8 
 * encoding, checks to see if the string is a continuation of the old string, and 
 * then splits it into an array based on the new line character.
 * it then makes a call to the main parser function and upon reaching a specified number
 * of documents for each patients, inserts them into the connected database
 * 
 * returns a promise
 */
parseVCF.prototype.readAndParse = function(chunk){
	var self = this;
	var buffer = new Buffer(chunk.chunkLength);
	return fs.readAsync(self.fd,buffer,0,chunk.chunkLength,chunk.chunkPosition)
		.then(function(buffer){
			return buffer[1].toString('utf-8',0,buffer[0]);
		}).then(function(string){
			if (self.oldString != ""){
				string = self.oldString + string;
			}
			splitString = string.split('\n');
			if (string.substr(string.length - 1) == "\n"){
				self.oldString = "";
			}  else {
				self.oldString = splitString.pop();
			}
			return splitString;
		}).then(function(stringArray){
			if (stringArray.length > 0 )
				return self.parseChunk(stringArray).map(function(patient){
					if (self.patientObj[patient]['documents'].length >= self.docMax)
						return self.checkAndInsertDoc(patient);
				});
		});
	};




/* Annovar multi-anno parser. Takes an array as input that contains lines separated by the
 * new line character. It then breaks the line into its an array by splitting on tabbed or
 * white space. It uses a mapper to link the headers with specific rows, as well as the mapped
 * patients indexes. It then outputs an object that it pushes to each patient in the 
 * this.patientObj.documents array.
 *
 * return a promise, and an array of patient keys
 */
parseVCF.prototype.parseChunk = function(stringArray){
	var self = this;
	var promise = new Promise(function(resolve,reject){
		//iterate over all strings that are included
		for (var i=0; i < stringArray.length ; i++ ){
			//If the file is malformed, it may have ##, ignore these
			if (stringArray[i] != "" && stringArray[i].slice(0,2) != "##"){
				line = stringArray[i].split('\t'); //split string
				//If this is the first line that has been read, extract the file headers
				if (self.first){
					line = line.filter(function(ele){
						if (ele != "Otherinfo")
							return ele; //return all but the Otherinfo field. This is not informative
					});
					for (var j=0; j<line.length; j++)
					{
						//add each field to the mapper, replacing periods or hashtags. the value is the corresponding columns
						self.mapper[line[j].replace(/\./gi,'_').replace(/#/i,"")] = j;
					}
					self.first = false;
				} else {
					if (self.firstEntry){ //first record
						var formatReached = false
						var formatNum;
						//iterate over the broken up line to find the rs number and the FORMATFIELD
						for (var j=0; j<line.length;j++){
							if (line[j].match(/^rs[0-9]+$/)){
								self.mapper['identifier'] = j;
							} else if (line[j].match(/^GT+/)) {
								self.mapper['FORMATFIELD'] = j;
								formatReached = true;
								formatNum = j + 1;
							} else if (formatReached) {
								//once formatField is reached, add the indexes of the patients to the patientObj
								self.patientObj[self.patients[j - formatNum]['patient_id']] = {'id':j,'collection':self.patients[j - formatNum]['collection_id'],'documents':[]};
								
							}	
						}
						self.firstEntry = false;
					}
					
					//using the mapper. add each field as an entry for each patient in the patientObj
					var formatMapper = [];
					var formatField = line[self.mapper['FORMATFIELD']].split(':');
					for (patient in self.patientObj){
						if (self.patientObj.hasOwnProperty(patient)){
							var currDoc = {};
							var formatLine = line[self.patientObj[patient]['id']].split(':');
							var cont = true;
							//extract information about the format of the current entry (can occasionally change so this)
							//Must be done each time.
							for (var j = 0; j < formatField.length; j++){
								var info = formatLine[j].split(/[\/|,]/);
								if (formatField[j] == 'GT'){
									if (info.indexOf('.') != -1){
										cont = false;
									} else {
										currDoc['GTRAW'] = formatLine[j];
										currDoc['GT'] = info;
										currDoc['phased_status'] = (formatLine[j].indexOf('|') != -1 || false);
									}
								} else if (cont){
									info = info.map(function(item){
										if (isNaN(item))
											return item;
										else
											return Math.round(item*1000)/1000;
									});
									
									currDoc[formatField[j]] = (info.length == 1 ? info[0]:info);
								}
							} // if the genotype given is ./. ie, there is nothing found for the patient, don't insert it
							  // Posibbly change to 0/0 ref allele later, and imply that any fields not included are ref/ref
							  // however that may be a bad idea
							if (cont) { 
								for (field in self.mapper){
									if (self.mapper.hasOwnProperty(field)){
										var thisLine = line[self.mapper[field]].split(/,/);
										if (field !== 'FORMATFIELD' && thisLine != '.'){
											//insert the other fields into the document
											var finalLine = thisLine.map(function(item){
												if (isNaN(item)){
													return item;
												} else { 
													return Math.round(item*1000)/1000; //convert to 3 decimal places
																					  //Add function later to change this
												}
											});
											currDoc[field] = (finalLine.length == 1 ? finalLine[0]:finalLine);
										}
									}
								}
								//when completed push this to the docuements array for each patient.
								self.patientObj[patient]['documents'].push(currDoc);
							}
						}
					}
				}
			}
		}
		resolve(Object.keys(self.patientObj));
	});
	return promise;
};


/* Upon completetion of the parsing, add any remaining entries into
 * the database. recursively calling itself until all entries are in
 */
parseVCF.prototype.cleanup = function(patient){
	var self = this;
	return this.checkAndInsertDoc(patient)
	.then(function(){
		if (self.patientObj[patient]['documents'].length > 0){
			self.cleanup(patient);
		}
	})
};


/* Insert up to this.docMax entries (default 999) into the connected database
 * for the specified patient. Additionally, remove from memory the documents 
 * that were inserted
 * returns a promise

 */
parseVCF.prototype.checkAndInsertDoc = function(patient){
	var self = this;
	var promise = new Promise(function(resolve,reject){
		var docsToInsert = self.patientObj[patient]['documents'];
		var ind = (docsToInsert.length > self.docMax ? self.docMax : docsToInsert.length);
		docsToInsert = docsToInsert.slice(0,ind);
		var options = {documents: docsToInsert,
				   collectionName:self.patientObj[patient]['collection']};
		
		dbFunctions.insertMany(options)
		.then(function(){
			self.patientObj[patient]['documents'] = self.patientObj[patient]['documents'].slice(ind);
			resolve(self.patientObj[patient]['documents'].length)
		}).catch(function(err){
			console.log(err);
		});
	});
	return promise;
};

module.exports = parseVCF;


